---
title: "Homework 3"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Timothy Choi"
date: "date here"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This homework runs logistic regression to predict the binary feature of whether or not a person was admitted to graduate school, based on a set of predictors: GRE score, TOEFL score, rating of undergrad university attended, SOP statement of purpose, LOR letter or recommendation, Undergrad GPA, Research experience (binary).

The data set was downloaded from Kaggle: https://www.kaggle.com/mohansacharya/graduate-admissions

The data is available in Piazza. 

## Step 1 Load the data

* Load the data
* Examine the first few rows with head()

```{r}
# your code here
df <- read.csv("Admission_Predict.csv")
head(df)
```

## Step 2 Data Wrangling

Perform the following steps:

* Make Research a factor
* Get rid of the Serial No column
* Make a new column that is binary factor based on if Chance.of.Admit > 0.5. Hint: See p. 40 in the book. 
* Output column names with names() function
* Output a summary of the data
* Is the data set unbalanced? Why or why not?

 Your commentary here:
 The data set is unbalanced because from the new data column we created a large majority of the data was TRUE and barely anything of them were FALSE. If there were a similar amount of TRUE and FALSE than the data set would be more balanced.

```{r}
# your code here
df$Research <- factor(df$Research)
df <- df[,2:9]
df$Admit <- FALSE
df$Admit[df$Chance.of.Admit > 0.5] <- TRUE
names(df)
```

```{r}
# put the summary here
summary(df)
```

## Step 3 Data Visualization

* Create a side-by-side graph with Admit on the x axis of both graphs, GRE score on the y axis of one graph and TOEFL score on the y axis of the other graph; save/restore the original graph parameters
* Comment on the graphs and what they are telling you about whether GRE and TOEFL are good predictors
* You will get a lot of warnings, you can suppress them with disabling warnings as shown below:

```
{r,warning=FALSE}
```

Your commentary here:

```{r,warning=FALSE}
# your code here
par(mfrow = c(1,2))
boxplot(df$Admit,df$GRE.Score, main = "Admit vs GRE score", xlab = "Admit", ylab = "GRE Score")
boxplot(df$Admit,df$TOEFL.Score, main = "Admit vs TOEFL Score", xlab = "Admit", ylab = "TOEFL Score")
```


## Step 4 Divide train/test

* Divide into 75/25 train/test, using seed 1234

```{r}
# your code here
set.seed(1234)
i <- sample(1:nrow(df), nrow(df)*.75, replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

## Step 5 Build a Model with all predictors 

* Build a model, predicting Admit from all predictors
* Output a summary of the model
* Did you get an error? Why? Hint: see p. 120 Warning

Your commentary here: 
Yes. The error was given because the trainning data was too perfect or nearly perfect.
```{r}
# your code here
m <- glm(Admit~GRE.Score+TOEFL.Score+University.Rating+SOP+LOR+CGPA+Research+Chance.of.Admit,data = train, family = "binomial")
summary(m)

```

## Step 6 Build a Model with all predictors except Chance.of.Admit

* Build another model, predicting Admit from all predictors *except* Chance.of.Admit
* Output a summary of the model
* Did you get an error? Why or why not?
No error was given because the column chance of admit was too good of a predictor but since we got rid of it the prediction is not as good.
```{r}
# your code here
m1 <- glm(Admit~GRE.Score+TOEFL.Score+University.Rating+SOP+LOR+CGPA+Research,data = train, family = "binomial")
summary(m1)
```

## Step 7 Predict probabilities

* Predict the probabilities using type="response"
* Examine a few probabilities and the corresponding Chance.of.Admit values
* Run cor() on the predicted probs and the Chance.of.Admit, and output the correlation
* What do you conclude from this correlation. 

Your commentary here:
The correlation tells us how accurate the probability is.
```{r}
# your code here
probs <- predict(m1, newdata = test, type = "response")
print(cor(probs, test$Chance.of.Admit))
```

## Step 8 Make binary predictions, print table and accuracy

* Now make binary predictions
* Output a table comparing the predictions and the binary Admit column
* Calculate and output accuracy
* Was the model able to generalize well to new data?

Your commentary here:

```{r}
# your code here
pred <- ifelse(probs > 0.5, 1, 0)
table(pred, test$Chance.of.Admit)
acc <- mean(pred == test$Admit)
print(paste("accuracy = ", acc))
```

## Step 9 Output ROCR and AUC

* Output a ROCR graph
* Extract and output the AUC metric

```{r}
# your code here
library(ROCR)
p <- predict(m1, newdata = test, type = "response")
pr <- prediction(p, test$Admit)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)
```


## Step 10

* Make two more graphs and comment on what you learned from each graph:
  * Admit on x axis, SOP on y axis
  * Research on x axis, SOP on y axis
  
Your commentary here:
In the first graph, for the people who did not get admitted there SOP was lower on average lower by 1 than those who did get admitted.

People who have research on average have a higher SOP than those who did not.

```{r}
# plot 1
boxplot(df$SOP~df$Admit, main = "Admit vs SOP", xlabel = "Admit", ylabel = "SOP")

```

```{r}
# plot 2
boxplot(df$SOP~df$Research, main = "Research vs SOP", xlabel = "Research", ylabel = "SOP")
```

