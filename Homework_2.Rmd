---
title: "Homework 2"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Timothy Choi"
date: "2-3-22"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This homework gives practice in using linear regression in two parts:

* Part 1 Simple Linear Regression (one predictor)
* Part 2 Multiple Linear Regression (many predictors)

You will need to install package ISLR at the console, not in your script. 

# Problem 1: Simple Linear Regression

## Step 1: Initial data exploration

* Load library ISLR (install.packages() at console if needed)
* Use names() and summary() to learn more about the Auto data set
* Divide the data into 75% train, 25% test, using seed 1234

```{r}
# your code here
library("ISLR")
names(Auto)
summary(Auto)
set.seed(1234)
i <- sample(1:nrow(Auto), nrow(Auto)*.75, replace=FALSE)
train <- Auto[i,]
test <- Auto[-i,]
```

## Step 2: Create and evaluate a linear model

* Use the lm() function to perform simple linear regression on the train data with mpg as the response and horsepower as the predictor
* Use the summary() function to evaluate the model 
* Calculate the MSE by extracting the residuals from the model like this: 
  mse <- mean(lm1$residuals^2)
* Print the MSE
* Calculate and print the RMSE by taking the square root of MSE

```{r}
# your code here
lm1 <- lm(mpg~horsepower, data = train)
summary(lm1)
mse <- mean(lm1$residuals^2)
print(mse)
rmse <- sqrt(mse)
print(rmse)
```

## Step 3 (No code. Write your answers in white space)

* Write the equation for the model, y = wx + b, filling in the parameters w, b and variable names x, y
* Is there a strong relationship between horsepower and mpg? 
*	Is it a positive or negative correlation? 
*	Comment on the RSE, R^2, and F-statistic, and how each indicates the strength of the model
*	Comment on the RMSE and whether it indicates that a good model was created
mpg = -0.156681 * horsepower + 39.648595

There is not that strong of a correlation because the r^2 value is not as close to 1 as we would like it to be.

There is a negative correlation because the w value is negative

Since the RSE is about 5, it tells us that the standard error of this linear regression is not to far off if we use it as a predictor. The R^2 value is shown to be 0.6136 because the value is so close to 0 it indicates the equation to be a good model. With the R^2 value being low, it means that the data does not stray to far off from the linear regression line. In the F-statistic, it shows that the p value is 2.2e-16 which means the value is closer to 0. The P value tells us whether or not we can reject the null hypothesis. Since the value is close to 0 we can reject the null hypothesis. All the these indications prove that the linear model is a strong representation.

The RMSE shows that we are about 5 units off when we compare the train and test data set. This shows that it is not the greatest model because of the differences in values.

## Step 4: Examine the model graphically

* Plot train\$mpg~train\$horsepower
* Draw a blue abline()
* Comment on how well the data fits the line
* Predict mpg for horsepower of 98. Hint: See the Quick Reference 5.10.3 on page 96
* Comment on the predicted value given the graph you created

Your commentary here:
The line fits the data for some of range but overall does not fit well. The data on the plot looks more like a 1/x curve than a linear curve.

that value 24.3 seems to be close to what is represented on the graph and linear regression line.
```{r}
# your code here
plot(train$mpg~train$horsepower)
abline(lm1, col = "blue")
mpg1 <- predict(lm1, data.frame(horsepower = 98))
```

## Step 5: Evaluate on the test data

* Test on the test data using the predict function
* Find the correlation between the predicted values and the mpg values in the test data
* Print the correlation
* Calculate the mse on the test results
* Print the mse
* Compare this to the mse for the training data
* Comment on the correlation and the mse in terms of whether the model was able to generalize well to the test data

Your commentary here:
The mse for the test data is even worse than the train data by 2. This tells us that the model is not as reliable as we would like it to be.

The correlation and mse tells us that the model can be trust a little about 50%. The reason so is because the correlation is about 76% right with the data and the mse was worse when we tested it compared to the train data.
```{r}
# your code here
pred2 <- predict(lm1, newdata = test)
mse1 <- mean((pred2 - test$mpg)^2)
cor1 <- cor(pred2, test$mpg)
print(cor1)
print(mse1)
```

## Step 6: Plot the residuals

* Plot the linear model in a 2x2 arrangement
* Do you see evidence of non-linearity from the residuals?

Your commentary here:
Yes. In the Residuals vs Fitted model, the line looks to be more of a parabola than a line. The Scale-Location model has linear lines but it seems to be more of a piece-wise function or a polynomial function and the Residuals vs Leverage
```{r}
# your code here
par(mfrow=c(2,2))
plot(lm1)
```

## Step 7: Create a second model

* Create a second linear model with log(mpg) predicted by horsepower
* Run summary() on this second model
* Compare the summary statistic R^2 of the two models

Your commentary here:
The R^2 in the second linear model is higher by almost .1. Therefore making the second linear model more accurate than the first model

```{r}
# your code here
lm2 <- lm(log(mpg)~horsepower, data = train)
summary(lm2)
```

## Step 8: Evaluate the second model graphically

* Plot log(train\$mpg)~train\$horsepower
* Draw a blue abline() 
* Comment on how well the line fits the data compared to model 1 above

Your commentary here:
The line looks to fit more of the data compared to the first one. When the data gets closer to 0, the data points seem to be more linear than curved.
```{r}
# your code here
plot(log(train$mpg)~train$horsepower)
abline(lm2, col = "blue")
```

## Step 9: Predict and evaluate on the second model

* Predict on the test data using lm2
* Find the correlation of the predictions and log() of test mpg, remembering to compare pred with log(test$mpg)
* Output this correlation
* Compare this correlation with the correlation you got for model 
* Calculate and output the MSE for the test data on lm2, and compare to model 1. Hint: Compute the residuals and mse like this:
```
residuals <- pred - log(test$mpg)
mse <- mean(residuals^2)
```

Your commentary here: 
The mse is for this model is extremely better than for the previous model because the value is much closer to 0 than 1 which indicates that the prediction is a good indication of what teh acutal data could be.
```{r}
# your code here
pred3 <- predict(lm2, newdata = test)
cor(pred3, log(test$mpg))
print(cor(pred3, log(test$mpg)))
mse2 <- mean((pred3 - log(test$mpg))^2)
print(mse2)
```

## Step 10: Plot the residuals of the second model

* Plot the second linear model in a 2x2 arrangement
* How does it compare to the first set of graphs?

Your commentary here:
For most of the graphs shown the data points seem to be more centralized around the line compared to the last one.
```{r}
# your code here
par(mfrow=c(2,2))
plot(lm2)
```

# Problem 2: Multiple Linear Regression

## Step 1: Data exploration

* Produce a scatterplot matrix of correlations which includes all the variables in the data set using the command “pairs(Auto)”
* List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.

Your commentary here:
positive                   negative
displacement~horsepower    mpg~displacement
displacement~weight        mpg~horsepower
horsepower~weight          mpg~weight
                           horsepower~acceleration
```{r}  
# your code here
pairs(Auto)

```


## Step 2: Data visualization

* Display the matrix of correlations between the variables using function cor(), excluding the “name” variable since is it qualitative
* Write the two strongest positive correlations and their values below. Write the two strongest negative correlations and their values as well.

Your commentary here:
positive:
cylinders~displacement = .9508233
displacement~weight = .9329944

negative:
mpg~weight = -0.8322442
mpg~displacement = -0.8051269

```{r}  
# your code here
c <- cor(Auto[1:8])
print(c)
```


## Step 3: Build a third linear model

* Convert the origin variable to a factor
* Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors
* Use the summary() function to print the results
* Which predictors appear to have a statistically significant relationship to the response?

Your commentary here:
The 2 values that have the most significant relationship is weight and year.
```{r} 
# your code here
Auto$origin <- factor(Auto$origin)
lm3 <- lm(Auto$mpg~Auto$cylinders + Auto$displacement + Auto$horsepower + Auto$weight + Auto$acceleration + Auto$year + Auto$origin, data = train)
summary(lm3)

```


## Step 4: Plot the residuals of the third model

* Use the plot() function to produce diagnostic plots of the linear regression fit
* Comment on any problems you see with the fit
* Are there any leverage points? 
* Display a row from the data set that seems to be a leverage point. 

Your commentary here:
At the end of the normal Q-Q the data seems the stray away from the line.

yes are leverage points.
```{r}  
# your code here
par(mfrow = c(2,2))
plot(lm3)
Auto[14,]
```


## Step 5: Create and evaluate a fourth model

* Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above
* Compare the summaries of the two models, particularly R^2
* Run anova() on the two models to see if your second model outperformed the previous one, and comment below on the results

Your commentary here: 
Base off the 2 summaries the one in step 5 was better because the R^2 value was about .04 better than the R^2 value in step 3.

The model I created in step 5 is better because it has a lower RSS and low p-value.
```{r}  
# your code here
lm4 <- lm(Auto$mpg~Auto$cylinders + Auto$displacement + Auto$horsepower + Auto$weight + Auto$acceleration + Auto$year + Auto$origin + Auto$displacement*Auto$weight + Auto$cylinders*Auto$displacement + Auto$displacement*Auto$horsepower + Auto$cylinders*Auto$weight, data = train)
summary(lm4)
anova(lm3,lm4)
```

